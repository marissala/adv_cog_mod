---
title: "Class4"
author: "Maris Sala"
date: "2/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#just coding loops
parameter recovery - still
```{r}
set.seed(1982)
setwd("~/Aarhus University/Advanced cog mod/progre")
library(R2jags)
```


Recover dif parameter combinations for the learning model
```{r}
#the alpha we put in
trueAlpha <- array(0,c(100))
inferredAlpha <- array(c(100)) #map

trueTheta <- array(0,c(100))
inferredTheta <- array(c(100))

ntrials <- 100

for (i in 1:100) {
  Glearn <- array(0, c(100)) #guess for our learning model
  theta <- array(0, c(100)) #theta is going to update every trial
  # we need a starting theta!
  alpha <- runif(1,0,1)#learning rate, 
  theta1 <- runif(1,0,1)
  
  #need a theta on trial 1, have to define the 1st trial, simulation runs from trial 2
  theta[1] <- theta1
  Glearn[1] <- rbinom(1,1,theta[1])
  
  for (t in 2:ntrials) {
    
    theta[t] <- theta[t-1]^(1/(1+alpha)) #this is the learning rule/curve, focus on this next week
    Glearn[t] <- rbinom(1,1,theta[t])
  
  }
  
  trueAlpha[i] <- alpha
  trueTheta[i] <- theta1
  
  # ---- run inference for the learning model using learning simulations
  G <- Glearn

  data <- list("G", "ntrials") #has to be a list in this case, look in the workspace for G
  params <- c("theta", "theta1", "alpha") #what are we interested in keeping track of, theta as a distribution
  
  samples <- jags(data, inits = NULL, params,
                  model.file = "jags_models/chick_learning.txt",
                  n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1)
  
  alpha.post <- samples$BUGSoutput$sims.list$alpha
  #max a posteriori for alpha
  inferredAlpha[i] <- density(alpha.post)$x[which(density(alpha.post)$y==max(density(alpha.post)$y) )]
  
  theta1.post <- samples$BUGSoutput$sims.list$theta1
  #max a posteriori for alpha
  inferredTheta[i] <- density(theta1.post)$x[which(density(theta1.post)$y==max(density(theta1.post)$y) )]
}


```

```{r}
plot(trueAlpha,inferredAlpha)
plot(trueTheta, inferredTheta)

#get plotting functions
source("quick_n_clean_plots.R")

#Beautiful plots
theta1_plot <- plot_actual_predicted(trueTheta, inferredTheta, caption = F)+
  ggtitle("Learning model: theta1") +
  theme_bw() +
  xlim(0,1) + ylim(0,1)

alpha_plot <- plot_actual_predicted(trueAlpha, inferredAlpha, caption = F)+
  ggtitle("Learning model: alpha") +
  theme_bw() +
  xlim(0,1) + ylim(0,1)

img_learning_model <-  gridExtra::grid.arrange(theta1_plot, alpha_plot, nrow=1)
```

Recover dif parameter combinations for the beta-binomial model
```{r}
#the alpha we put in
trueTheta <- array(0,c(100))
inferredTheta <- array(c(100))

ntrials <- 100

for (i in 1:100) {
  Gfixed <- array(0, c(100)) #guess for our learning model
  theta <- .7 #theta is fixed
  
  for (t in 1:ntrials) {
    Gfixed[t] <- rbinom(1,1,theta) #each element in the array populated with consequences of the model
  }

  trueTheta[i] <- theta
  
  # ---- run inference for the learning model using learning simulations
  G <- Gfixed

  data <- list("G", "ntrials") #has to be a list in this case, look in the workspace for G
  params <- c("theta") #what are we interested in keeping track of, theta as a distribution
  
  samples <- jags(data, inits = NULL, params,
                model.file = "jags_models/chick.txt",
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1) #inits could be important when there are convergence issues
  
  theta.post <- samples$BUGSoutput$sims.list$theta
  #max a posteriori for alpha
  inferredTheta[i] <- density(theta.post)$x[which(density(theta.post)$y==max(density(theta.post)$y) )]
}


```

Visualize
```{r}
plot(trueTheta, inferredTheta)

#get plotting functions
source("quick_n_clean_plots.R")

#Beautiful plots
plot_actual_predicted(trueTheta, inferredTheta, caption = F)+
  ggtitle("Beta-binomial model: theta") +
  theme_bw() +
  xlim(0,1) + ylim(0,1)

plot(density(inferredTheta))

d <- density(inferredTheta)
plot(d, main="Beta-binomial model: density of inferred theta")
polygon(d, col="lightblue", border="lightblue")
median(inferredTheta)
```


Model recovery!
have to have 1 model thats better than the other bc we are using the DIC to compare 2 models
use the fixed model and learning model as a control against each other

```{r}

#model comparison - homework

## MODEL 1: Fixed Theta model ##
Gfixed <- array(0, c(100)) #100 elements of 0s
#array(0, c(2,100)) if theres 2 agents for example
#empty array we will fill up with a simple forward model

#need a fixed parameter
theta <- .7 #anything thats less than 0 more than 1 is not legit
#someone who is okay at this task

ntrials <- 100

for (t in 1:ntrials) {
  Gfixed[t] <- rbinom(1,1,theta) #each element in the array populated with consequences of the model
}

# MODEL 2
Glearn <- array(0, c(100)) #guess for our learning model
theta <- array(0, c(100)) #theta is going to update every trial
# we need a starting theta!
alpha <- .05 #learning rate, can never be interpreted contextually free
theta1 <- .5

ntrials <- 100

#need a theta on trial 1, have to define the 1st trial, simulation runs from trial 2
theta[1] <- theta1
Glearn[1] <- rbinom(1,1,theta[1])

for (t in 2:ntrials) {
  
  theta[t] <- theta[t-1]^(1/(1+alpha)) #this is the learning rule/curve, focus on this next week
  Glearn[t] <- rbinom(1,1,theta[t])

}


# -- run inference for fixed data and fixed model
G <- Gfixed

data <- list("G", "ntrials") #has to be a list in this case, look in the workspace for G
params <- c("theta") #what are we interested in keeping track of, theta as a distribution

samples <- jags(data, inits = NULL, params,
                model.file = "jags_models/chick.txt",
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1) #inits could be important when there are convergence issues

DIC.fixed_fixed <- samples$BUGSoutput$DIC

# put fixed data in the learning model
G <- Glearn

data <- list("G", "ntrials") #has to be a list in this case, look in the workspace for G
params <- c("theta") #what are we interested in keeping track of, theta as a distribution

samples <- jags(data, inits = NULL, params,
                model.file = "jags_models/chick.txt", #DONT CHANGE THE FILE HERE!
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1) #inits could be important when there are convergence issues

DIC.learning_fixed <- samples$BUGSoutput$DIC

# fixed data to learning model
G <- Gfixed

data <- list("G", "ntrials") #has to be a list in this case, look in the workspace for G
params <- c("theta") #what are we interested in keeping track of, theta as a distribution

samples <- jags(data, inits = NULL, params,
                model.file = "jags_models/chick_learning.txt", #DONT CHANGE THE FILE HERE!
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1) #inits could be important when there are convergence issues

DIC.fixed_learning <- samples$BUGSoutput$DIC


# fixed data to learning model
G <- Glearn

data <- list("G", "ntrials") #has to be a list in this case, look in the workspace for G
params <- c("theta") #what are we interested in keeping track of, theta as a distribution

samples <- jags(data, inits = NULL, params,
                model.file = "jags_models/chick_learning.txt", #DONT CHANGE THE FILE HERE!
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1) #inits could be important when there are convergence issues

DIC.learning_learning <- samples$BUGSoutput$DIC


DIC.fixed_fixed
DIC.learning_fixed
DIC.fixed_learning
DIC.learning_learning

```
DIC fixed should be lower than DIC learning because its its own model - so this is crap model recovery
make a confusion matrix!

```{r}
pacman::p_load(caret)

predicted <- array(0, c(1,2))
predicted[1,1] = DIC.learning_fixed
predicted[1,2] = DIC.fixed_learning

actual <- array(0, c(1,2))
actual[1,1] <- DIC.fixed_fixed
actual[1,2] <- DIC.learning_learning

predicted = as.data.frame(predicted)
predicted$V1 = as.factor(predicted$V1)
predicted$V2 = as.factor(predicted$V2)
actual = as.data.frame(actual)
actual$V1 = as.factor(actual$V1)
actual$V2 = as.factor(actual$V2)

x <- factor(c(DIC.learning_fixed, DIC.fixed_learning))
y <- factor(c(DIC.fixed_fixed, DIC.learning_learning))

confusionMatrix(x,y)
```

Ludvig's package for confusion matrix
```{r}
pacman::p_load(broom, tibble)
library(cvms)

# 0 is best model
best_kernel
bk = replace(best_kernel, best_kernel==1, 0)
bk = replace(bk, bk == 2, 1)

# 1 is best model
best_RW
brw = replace(best_RW, best_RW==2, 0)

# combine
tg = c(brw, bk)

# Results in a tibble
d_confusion <- tibble("target" = c(runif(length(best_RW),1,1), runif(length(best_kernel),0,0)),
                      "prediction" = tg)

cfm <- tidy(table(d_confusion))

plot_confusion_matrix(cfm,
                      targets_col = "target",
                      predictions_col = "prediction",
                      counts_col = "n")

```