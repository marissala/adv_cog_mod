---
title: "Class10"
author: "Maris Sala"
date: "3/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the PVL assignment done independently under corona lockdown

THE ORL MODEL

```{r}
set.seed(1982)
#setwd("~/Aarhus University/Advanced cog mod/progre")
pacman::p_load(extraDistr, R2jags)
```

Task environment - same as for the IGT model in Class9

```{r}
# Bad frequent
A_R <- rep(100,10) #reward for deck A, 10 outcomes
A_L <- c(rep(-250,5), rep(0,5)) #losses 50% of the times
# Bad infrequent
B_R <- rep(100,10)
B_L <- c(rep(-1250,1), rep(0,9)) #one big loss but very infrequent
# Good frequent
C_R <- rep(50,10)
C_L <- c(rep(-50,5), rep(0,5))
# Good infrequent
D_R <- rep(50,10)
D_L <- c(rep(-250,1), rep(0,9))

# For the forward simulations: shuffle the deck
# Set up the environment with separate wins and losses as if they will be presented separately to the participants
A <- c()
for (i in 1:10) { A <- (append(A,A_R + sample(A_L))) } # To each of the 4 decks, adds losses and rewards together

B <- c()
for (i in 1:10) { B <- (append(B,B_R + sample(B_L))) }

C <- c()
for (i in 1:10) { C <- (append(C,C_R + sample(C_L))) }

D <- c()
for (i in 1:10) { D <- (append(D,D_R + sample(D_L))) }

# The sums of taking only from one deck: -2500 or +2500
sum(A)
sum(B)
sum(C)
sum(D)

# Payoff matrix
payoff <- cbind(A,B,C,D)
```

Build the ORL model
```{r}
# Learning rate for rewards
a_rew <- 0.9
# Learning rate for punishments
a_pun <- .4
# Weight1
beta_f <- 10
# Weight2
beta_p <- 10
# Decay parameter for perseverance
K <- 1
# Softmax inverse heat
theta <- 1

ntrials <- 100

source("R_fun/ORL.R")
ORL_sims <- ORL(payoff, ntrials, a_rew, a_pun, beta_f, beta_p, K, theta)

par(mfrow=c(2,2))
plot(ORL_sims$Ev[,1])
plot(ORL_sims$Ev[,2])
plot(ORL_sims$Ev[,3])
plot(ORL_sims$Ev[,4])

par(mfrow=c(2,2))
plot(ORL_sims$Ef[,1])
plot(ORL_sims$Ef[,2])
plot(ORL_sims$Ef[,3])
plot(ORL_sims$Ef[,4])

par(mfrow=c(2,2))
plot(ORL_sims$V[,1])
plot(ORL_sims$V[,2])
plot(ORL_sims$V[,3])
plot(ORL_sims$V[,4])

par(mfrow=c(2,2))
plot(ORL_sims$PS[,1])
plot(ORL_sims$PS[,2])
plot(ORL_sims$PS[,3])
plot(ORL_sims$PS[,4])

plot(ORL_sims$r)
plot(ORL_sims$x)


```

Fit the jags model
```{r}
x <- ORL_sims$x
r <- ORL_sims$r

# Learning rate for rewards
a_rew <- 0.9
# Learning rate for punishments
a_pun <- .4
# Weight1
beta_f <- 10
# Weight2
beta_p <- 10
# Decay parameter for perseverance
K <- 1
# Softmax inverse heat
theta <- 1

data <- list("x", "r", "ntrials")
params <- c("a_rew", "a_pun", "beta_f", "beta_p", "K", "theta")

samples <- jags.parallel(data, inits = NULL, params,
                model.file = "jags_models/ORL.txt",
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1)

a_rew_infer <- samples$BUGSoutput$sims.list$a_rew
a_pun_infer <- samples$BUGSoutput$sims.list$a_pun
beta_f_infer <- samples$BUGSoutput$sims.list$beta_f
beta_p_infer <- samples$BUGSoutput$sims.list$beta_p
K_infer <- samples$BUGSoutput$sims.list$K
theta_infer <- samples$BUGSoutput$sims.list$theta

#original numbers
w <- .1 # Loss aversion (2 from Khaneman)
A <- .5 # Shape of the utility function, between 0 and 1 to have the proper shape
theta <- 2 # Response consistency
a <- .01 # Learning rate

#-------Plot prospect theory function - code to play around with
w <- 2
A <- .2
x <- seq(1,100,1) #x axis #objective value
y <- x^A #subjective value
plot(x,y) # Utility function

par(mfrow=c(1,1))
plot(density(a_infer))
plot(density(w_infer))
plot(density(A_infer))
plot(density(theta_infer))
```

