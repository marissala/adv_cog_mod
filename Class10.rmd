---
title: "Class10"
author: "Maris Sala"
date: "3/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the PVL assignment done independently under corona lockdown

THE ORL MODEL

```{r}
set.seed(1982)
#setwd("~/Aarhus University/Advanced cog mod/progre")
pacman::p_load(extraDistr, R2jags)
```

Task environment - same as for the IGT model in Class9

```{r}
# Bad frequent
A_R <- rep(100,10) #reward for deck A, 10 outcomes
A_L <- c(rep(-250,5), rep(0,5)) #losses 50% of the times
# Bad infrequent
B_R <- rep(100,10)
B_L <- c(rep(-1250,1), rep(0,9)) #one big loss but very infrequent
# Good frequent
C_R <- rep(50,10)
C_L <- c(rep(-50,5), rep(0,5))
# Good infrequent
D_R <- rep(50,10)
D_L <- c(rep(-250,1), rep(0,9))

# For the forward simulations: shuffle the deck
# Set up the environment with separate wins and losses as if they will be presented separately to the participants
A <- c()
for (i in 1:10) { A <- (append(A,A_R + sample(A_L))) } # To each of the 4 decks, adds losses and rewards together

B <- c()
for (i in 1:10) { B <- (append(B,B_R + sample(B_L))) }

C <- c()
for (i in 1:10) { C <- (append(C,C_R + sample(C_L))) }

D <- c()
for (i in 1:10) { D <- (append(D,D_R + sample(D_L))) }

# The sums of taking only from one deck: -2500 or +2500
sum(A)
sum(B)
sum(C)
sum(D)

# Payoff matrix
payoff <- cbind(A,B,C,D)
```

Build the ORL model
```{r}
# Learning rate for rewards
a_rew <- 0.1
# Learning rate for punishments
a_pun <- .4
# Weight1
beta_f <- 1 #-3 and 3
# Weight2
beta_p <- 1.5
# Decay parameter for perseverance
K <- 1
# Softmax inverse heat
theta <- 1

ntrials <- 100

source("R_fun/ORL.R")
ORL_sims <- ORL(payoff/100, ntrials, a_rew, a_pun, beta_f, beta_p, K, theta)

par(mfrow=c(2,2))
plot(ORL_sims$Ev[,1])
plot(ORL_sims$Ev[,2])
plot(ORL_sims$Ev[,3])
plot(ORL_sims$Ev[,4])

par(mfrow=c(2,2))
plot(ORL_sims$Ef[,1])
plot(ORL_sims$Ef[,2])
plot(ORL_sims$Ef[,3])
plot(ORL_sims$Ef[,4])

par(mfrow=c(2,2))
plot(ORL_sims$V[,1])
plot(ORL_sims$V[,2])
plot(ORL_sims$V[,3])
plot(ORL_sims$V[,4])

par(mfrow=c(2,2))
plot(ORL_sims$PS[,1])
plot(ORL_sims$PS[,2])
plot(ORL_sims$PS[,3])
plot(ORL_sims$PS[,4])

plot(ORL_sims$r)
plot(ORL_sims$x)


```

Fit the jags model
```{r}
x <- ORL_sims$x
r <- ORL_sims$r

# Learning rate for rewards
a_rew <- 0.2
# Learning rate for punishments
a_pun <- .4
# Weight1
beta_f <- 1
# Weight2
beta_p <- 1.5
# Decay parameter for perseverance
K <- 1
# Softmax inverse heat
theta <- 1

data <- list("x", "r", "ntrials")
params <- c("a_rew", "a_pun", "beta_f", "beta_p", "K", "theta")

samples <- jags.parallel(data, inits = NULL, params,
                model.file = "jags_models/ORL.txt",
                n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1)

a_rew_infer <- samples$BUGSoutput$sims.list$a_rew
a_pun_infer <- samples$BUGSoutput$sims.list$a_pun
beta_f_infer <- samples$BUGSoutput$sims.list$beta_f
beta_p_infer <- samples$BUGSoutput$sims.list$beta_p
K_infer <- samples$BUGSoutput$sims.list$K
theta_infer <- samples$BUGSoutput$sims.list$theta

#original numbers
a_rew <- 0.2
a_pun <- .4
beta_f <- 1
beta_p <- 1.5
K <- 1
theta <- 1

par(mfrow=c(3,3))
plot(density(a_rew_infer))
plot(density(a_pun_infer))
plot(density(beta_f_infer))
plot(density(beta_p_infer))
plot(density(K_infer))
plot(density(theta_infer))
```

Parameter recovery
```{r}
niterations <- 5 #30 later
ORL_t.a_rew <- array(0, c(niterations))
ORL_t.a_pun <- array(0, c(niterations))
ORL_t.beta_f <- array(0, c(niterations))
ORL_t.beta_p <- array(0, c(niterations))
ORL_t.K <- array(0, c(niterations))
ORL_t.theta <- array(0, c(niterations))

ORL_i.a_rew <- array(0, c(niterations))
ORL_i.a_pun <- array(0, c(niterations))
ORL_i.beta_f <- array(0, c(niterations))
ORL_i.beta_p <- array(0, c(niterations))
ORL_i.K <- array(0, c(niterations))
ORL_i.theta <- array(0, c(niterations))

##
ntrials <- 100

for (i in 1:niterations) {
  # true parameters
  a_rew <- runif(1,0,1)
  a_pun <- runif(1,0,1)
  beta_f <- rnorm(1,0,5) #not sure here
  beta_p <- rnorm(1,0,5) #not sure here
  K <- runif(1,0,1)
  theta <- runif(1,0,1)
  
  #run function and extract responses
  source("R_fun/ORL.R")
  ORL_sims <- ORL(payoff,ntrials,a_rew, a_pun, beta_f, beta_p, K, theta)
  x <- ORL_sims$x
  r <- ORL_sims$r
  
  data <- list("x", "r", "ntrials")
  params <- c("a_rew", "a_pun", "beta_f", "beta_p", "K", "theta")

  samples <- jags.parallel(data, inits = NULL, params,
                  model.file = "jags_models/ORL.txt",
                  n.chains = 3, n.iter = 5000, n.burnin = 1000, n.thin = 1)
  
  ORL_t.a_rew[i] <- a_rew
  ORL_t.a_pun[i] <- a_pun
  ORL_t.beta_f[i] <- beta_f
  ORL_t.beta_p[i] <- beta_p
  ORL_t.K[i] <- K
  ORL_t.theta[i] <- theta
  
  #find max posteriori
  X <- samples$BUGSoutput$sims.list$a_rew
  ORL_i.a_rew[i] <- density(X)$x[which(density(X)$y==max(density(X)$y))]
  
  X <- samples$BUGSoutput$sims.list$a_pun
  ORL_i.a_pun[i] <- density(X)$x[which(density(X)$y==max(density(X)$y))]
  
  X <- samples$BUGSoutput$sims.list$beta_f
  ORL_i.beta_f[i] <- density(X)$x[which(density(X)$y==max(density(X)$y))]
  
  X <- samples$BUGSoutput$sims.list$beta_p
  ORL_i.beta_p[i] <- density(X)$x[which(density(X)$y==max(density(X)$y))]
  
  X <- samples$BUGSoutput$sims.list$K
  ORL_i.K[i] <- density(X)$x[which(density(X)$y==max(density(X)$y))]
  
  X <- samples$BUGSoutput$sims.list$theta
  ORL_i.theta[i] <- density(X)$x[which(density(X)$y==max(density(X)$y))]
  
  print(i)
}

plot(ORL_t.a_rew, ORL_i.a_rew)
plot(ORL_t.a_pun, ORL_i.a_pun)
plot(ORL_t.beta_f, ORL_i.beta_f)
plot(ORL_t.beta_p, ORL_i.beta_p)
plot(ORL_t.K, ORL_i.K)
plot(ORL_t.theta, ORL_i.theta)
```

