---
title: "Class10"
author: "Maris Sala"
date: "3/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the PVL assignment done independently under corona lockdown

THE ORL MODEL

```{r}
set.seed(1982)
#setwd("~/Aarhus University/Advanced cog mod/progre")
pacman::p_load(extraDistr, R2jags)
```

Task environment - same as for the IGT model in Class9

```{r}
# Bad frequent
A_R <- rep(100,10) #reward for deck A, 10 outcomes
A_L <- c(rep(-250,5), rep(0,5)) #losses 50% of the times
# Bad infrequent
B_R <- rep(100,10)
B_L <- c(rep(-1250,1), rep(0,9)) #one big loss but very infrequent
# Good frequent
C_R <- rep(50,10)
C_L <- c(rep(-50,5), rep(0,5))
# Good infrequent
D_R <- rep(50,10)
D_L <- c(rep(-250,1), rep(0,9))

# For the forward simulations: shuffle the deck
# Set up the environment with separate wins and losses as if they will be presented separately to the participants
A <- c()
for (i in 1:10) { A <- (append(A,A_R + sample(A_L))) } # To each of the 4 decks, adds losses and rewards together

B <- c()
for (i in 1:10) { B <- (append(B,B_R + sample(B_L))) }

C <- c()
for (i in 1:10) { C <- (append(C,C_R + sample(C_L))) }

D <- c()
for (i in 1:10) { D <- (append(D,D_R + sample(D_L))) }

# The sums of taking only from one deck: -2500 or +2500
sum(A)
sum(B)
sum(C)
sum(D)

# Payoff matrix
payoff <- cbind(A,B,C,D)
```

Build the ORL model
```{r}
# Learning rate for rewards
a_rew <- .2
# Learning rate for punishments
a_pun <- .4
# Weight1
beta_f <- 10
# Weight2
beta_p <- -10
# Decay parameter for perseverance
K <- 2
# Softmax inverse heat
theta <- 1

ntrials <- 100

source("R_fun/ORL.R")
PVL_sims <- ORL(payoff, ntrials, a_rew, a_pun, beta_f, beta_p, K, theta)

par(mfrow=c(2,2))
plot(PVL_sims$Ev[,1])
plot(PVL_sims$Ev[,2])
plot(PVL_sims$Ev[,3])
plot(PVL_sims$Ev[,4])
```
```{r}

```

