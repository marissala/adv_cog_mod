model {

  #random priors
  a_rew ~ dnorm(0,1)T(0,1)
  a_pun ~ dnorm(0,1)T(0,1) #both at the bottom and at the top - almost uniform; they can't be risk seeking for gains, and risk averse for losses (assuming prospect theory)
  beta_f ~ dnorm(0,1)
  beta_p ~ dnorm(0,1) #uniform but for calculation reasons we do this
  K ~ dnorm(0,1)T(0,)
  theta ~ dnorm(0,1)T(0,1)
  
  C <- 3
  
  # gut feelings on 1st trial
  p[1, 1:4] <- c(0.25, 0.25, 0.25, 0.25)
  Ev[1, 1:4] <- c(1, 1, 1, 1)
  Ef[1, 1:4] <- c(1, 1, 1, 1)
  PS[1, 1:4] <- c(1, 1, 1, 1)
  V[1, 1:4] <- c(1, 1, 1, 1)
  
  # Building the model
  for (t in 2:ntrials) {
    
    # New variable that tracks the sign
    signX[t-1] <- ifelse(r[t-1] == 0, 0, ifelse(r[t-1] > 0, 1, -1))

    
    for (d in 1:4) {
      
      # Ev_update now uses a different learning rate for wins and losses
      Ev_update[t,d] <- ifelse(r[t-1] >= 0, 
                               (Ev[t-1,d] + (a_rew * (r[t-1] - Ev[t-1,d]))), 
                               (Ev[t-1,d] + (a_pun * (r[t-1] - Ev[t-1,d]))))
      
      # This remains the same as in PVL-delta
      Ev[t,d] <- ifelse(x[t-1] == d, 
                        Ev_update[t,d], 
                        Ev[t-1,d])
      #if we are on the same deck as we are updating then update ev, otherwise keep last value?
      
      # Frequency, use signX instead of magnitude, also need separate to rewards and losses
      Ef_chosen[t,d] <- ifelse(r[t-1] > 0, 
                               Ef[t-1,d] + (a_rew * (signX[t-1] - Ef[t-1,d])), 
                               Ef[t-1,d] + (a_pun * (signX[t-1] - Ef[t-1,d])))
      
      Ef_not[t,d] <- ifelse(r[t-1] > 0, 
                            Ef[t-1,d] + (a_pun * ( -signX[t-1]/C - Ef[t-1,d])), 
                            Ef[t-1,d] + (a_rew * (-signX[t-1]/C - Ef[t-1,d])))
      
      # Ef updating is similar to Ev updating
      Ef[t,d] <- ifelse(x[t-1] == d, 
                        Ef_chosen[t,d], 
                        Ef_not[t,d])
      
      # Perseverance
      PS[t,d] <- ifelse(x[t-1] == d, 
                        1/(1 + K), 
                        PS[t-1,d]/(1+K))
      
      # Linear combination V variable calculations, same as linear regression
      V[t,d] <- Ev[t,d] + Ef[t,d] * beta_f + PS[t,d] * beta_p
      
      # Calculate the softmax
      exp_p[t,d] <- exp(theta * V[t,d]) # can leave theta out since its fixed at 1
      #softmax
      
    }
    
    for (d in 1:4) {
      p[t,d] <- exp_p[t,d] / sum(exp_p[t,])
    }
    
    x[t] ~ dcat(p[t,]) #sample
    
  }
  
}